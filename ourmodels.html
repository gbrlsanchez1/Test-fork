<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="shortcut icon" href="images/harvard.ico" type="image/vnd.microsoft.icon" />
    <meta name="description" content="Cs109a datascience userratings team project : Team project - Applying Data Science best-practices to user-ratings.  Team members: Timur Zambalayev and Joshua Coffie">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>CS-109a - Intro to Datascience Team Project</title>
  </head>

  <body>

    <!-- HEADER -->
    <style>
    ul {
        list-style-type: none;
        margin: 0;
        padding: 0;
        overflow: hidden;
        background-color: #333;
    }

    li {
        float: left;
    }

    li a {
        display: block;
        color: white;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
    }

    li a:hover {
        background-color: #111;
    }

    .active {
    background-color: #0090ff;
    }

    </style>
    </head>
    <body>

    <ul>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/index.html">Home</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourdataset.html">Dataset</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/datacleanup.html">Data Cleanup</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourbaselines.html">Baselines</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourapproach.html">Approach</a></li>
      <li><a class="active" href="/CS109a_DataScience_UserRatings_Team_Project/ourmodels.html">Our Models</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/references.html">References</a></li>
    </ul>

    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project">View on GitHub</a>

          <h1 id="project_title">CS-109a - Intro to Datascience Team Project</h1>
          <h2 id="project_tagline">More on our Models</h2>


            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Continued Exploration</h3>

<p>After reviewing the Netflix prize submission paper, we realized that there are quite a few nuances that our data plays to that the Netflix data doesn't.  In order to get a grasp on the differences, we wanted to run some additional analysys.  Firstly, we generated 30 random pairs of users and explored their shared ratings.  In other words, are users rating the same movies so that we input from multiple people or are users fairly independent and only review different movies.</p>

<div style="display: flex; justify-content: center;">
  <img src="images/30_random_pairs.png", alt="30 random pairs", style="width:600px;height:600px;" />
</div>

<p>The result, as you can see in the image above, is that the mean number of shared movies from this sampling was just over 18 with a range from 0 - 126 shared movies.  This is fairly concerning, from a prediction standput, as we can really only expect our model to perform so well, given that there isn't much overlap between reviews, users, and movies.  Viewing the 30 random samplings in a histogram also shows us that the distribution isn't normal, but very skewed toward a bin of 25.2.  The data is so skewed, in fact, that this bin is larger than all of the other bins combined.</p>

<p>Some other interesting differences between our dataset from MovieLens and the Netflix prize dataset:</p>
<p>1. The Netflix dataset has <strong>substantially more user input than the number of movies</strong> (users stating their preference, let's say).  The exact opposite is true for our dataset, as the number of movies (~1700) is greater than the number of users submitting reviews (~1k).  This is something we can expect to have an impact on our model.</p>
<p>2. The movies in <strong>the Netflix data have 20+ reviews per movie</strong>.  In our dataset, we have nearly 50% of the movies with only 1 or 2 reviews.  We can expect this to have an impact on our model as well.</p>
<p>3. The Netflix solution was also able to work with <strong>non-negative coefficients</strong>, which our model was not able to do.  We believe this is because of the structure of our dataset (number of reviews per movie combined with a lack of overlap with reviews).</p>
<p>4. As a final note, the winning submission for the Netflix Prize blended over 100 models into their final solution.  For the purposes of this project, and the dataset at hand, that would be hard to do since <strong>we don't have nearly as many predictors in our MovieLens dataset</strong> to create that many models.  As a result, we are constrained to the number of predictors available in our dataset, unless we find a complementing dataset we can merge with our own that offers additional items we can leverage individual effects with.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Euclidean, Manhattan, and Pearson Correlation</h3>

<p>Our first attempt at a model was distance based, relying on Euclidean, Manhattan, and Pearson correlation-based modeling.  In a nutshell, these models compare 30 random pairs to determine how well of a fit we were able to create; a distance of 1 suggests a close fit while a distance of 0 is about as far away as you can get.</p>
<p>This approach, while a good starting point, wasn't particularly successful, as the mean distance for each of the models was: <strong>Euclidean</strong>- .195, <strong>Manhattan</strong>- .135, and <strong>Pearson</strong>- .207.</p>
<p>If you're interested in the code, give this <a href="documents/Final+Milestone.pdf">pdf</a> a view or visit the Notebook directly <a href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/blob/master/Final%20Milestone/MovieLens/Final%20Milestone.ipynb">here</a>.
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>User Effects and Item Effects and Blending them Together</h3>

<p>Our second set of models took user effects and item effects independently and then blended.  When taken independently, the models barely registered at the same level as our baseline models.  What's interesting, however, is that independently, the models performed pretty poorly but when combined, the score rose considerably (.23).  We also took into account the possibility that portions of the dataset might be better served using the baseline prediction instead of our model.  Combining all of these methods together, and blending our models into a single solution, still only gave us a score of .26.</p>
<p>If you're interested in the code, take a look at what's available <a href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/tree/master/Final%20Milestone/MovieLens">here.</a>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>UV Decomposition</h3>

<p>This seems to be a favorite topic among the data scientists of the World.  We implemented UV composition across a portion of the dataset (let's say 5%), and the time that it took the model to run was severe.  As a whole, training and predicting using our current dataset would have taken hours to process through the model.  For this reason, we decided that although the findings would have been interesting to report here, the processing effort required train and test the model, with the results we had on a portion of the dataset, did not justify its continued use.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Our Winning Model!</h3>

<p>Our winning model for the project used <strong>blending of baseline values and k-nn (k=30) with item effects (movie similarity)</strong>, where baseline values accounted for 7% of the datapoints.  This model gave us our highest test score of .3045 and a Root Mean Square Error (RMSE) of .8776, taking 5 sec to initiate the effects, 1 min 21 sec to fit the data, and 11 min to score the model.</p>
<p>Now that we have the model that seems to best represent our dataset, let's go through the process of understanding how to effectively rate a recommendation engine.  If you think back to the approach page, here are the areas we need to analyze our model, along with how these areas are typically analyzed (even if we don't have the ability to explore the model this in depth for the project, it's good for us to go through the motions of understanding next steps toward implementation):</p>

<p>1. <strong>User Preference-</strong> typically determined by user interviews and surveys.  Simply posing the question, "which model do you prefer?" </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- This process is out of scope for the project but a very interesting and essential test of any model - let's run this by users and see if their gut reactions and feelings/preferences match what the data are telling us.</p>
<p>2. <strong>Prediction Accuracy-</strong> normally measured with R^2 or a similar error explanation term, this can also take into account % classified properly (within  given margin).</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- In this case, let's compare our performance against the baseline as well as our scoring.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- Compared to the baseline, we've ... (R^2)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- Next question - were the gains worth the processing time and effort to build and implement the model?</p>
<p>3. <strong>Coverage-</strong> measured by the percentage of all items that can ever be recommended given our dataset.  For instance, if our model can only correctly predict 25% of the dataset, our coverage is only 25%.  And in that example, we would only use the model to predict that portion of the dataset (i.e., not recommend movies outside of our model's capability and not attempt a recommendation on a similar movie/rating/user who may not fall within those guidelines as well)</p>
<p>4. <strong>Confidence-</strong> (the system’s trust in its recommendations or predictions, which can grow with the amount of data it has to work with)</p>
<p>5. <strong>Trust-</strong> (the user’s trust in the system’s recommendations, which may lead us to introduce prior beliefs to lead the system to recommend “safer” options for users so that it is “right” more often than not)</p>
<p>6. <strong>Novelty-</strong> (recommendations for items that the user did not know about)</p>
<p>7. <strong>Serendipity-</strong> (how surprising the successful recommendations are to users)</p>
<p>8. <strong>Diversity-</strong> (a measure of item-to-item similarity – recommending the same movie a user likes, for instance, isn’t particularly helpful)</p>
<p>9. <strong>Utility-</strong> (the expected utility of the recommendation – does this recommendation engine result in the service being used more, for instance)</p>
<p>10. <strong>Risk-</strong> (minimizing risk in recommending items)</p>
<p>11. <strong>Robustness-</strong> (the stability of the recommendation in the presence of fake information)</p>
<p>12. <strong>Privacy-</strong> (no disclosure of private information from the user)</p>
<p>13. <strong>Adaptivity-</strong> (how well the model adapts to changing title availability and trends in preference)</p>

<h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Continuing our findings</h3>

<p>To continue working through this dataset to find the ultimate model, we should consider the following approaches:</p>

<p>1. <strong>Idea #1-</strong> what this is and might mean</p>
<p>2. <strong>Idea #2-</strong> what this is and might mean</p>
<p>3. <strong>Remove additional global effects-</strong> just as we pulled out user and movie effects to build our models, we could continue doing the same process with actors, directors, genre, and more.  We could emulate the rating using temporal information (ratings will decline as movies get older, for instance).  This proces could reveal some interesting information for our modeling process.</p>



      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">This page maintained by <a href="https://github.com/starbuck10">starbuck10</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a> &copy; 2016</p>
      </footer>
    </div>



  </body>
</html>
