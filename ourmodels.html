<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="shortcut icon" href="images/harvard.ico" type="image/vnd.microsoft.icon" />
    <meta name="description" content="Cs109a datascience userratings team project : Team project - Applying Data Science best-practices to user-ratings.  Team members: Timur Zambalayev and Joshua Coffie">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>CS-109a - Intro to Datascience Team Project</title>
  </head>

  <body>

    <!-- HEADER -->
    <style>
    ul {
        list-style-type: none;
        margin: 0;
        padding: 0;
        overflow: hidden;
        background-color: #333;
    }

    li {
        float: left;
    }

    li a {
        display: block;
        color: white;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
    }

    li a:hover {
        background-color: #111;
    }

    .active {
    background-color: #0090ff;
    }

    </style>
    </head>
    <body>

    <ul>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/index.html">Home</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourdataset.html">Dataset</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/datacleanup.html">Data Cleanup</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourbaselines.html">Baselines</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/ourapproach.html">Approach</a></li>
      <li><a class="active" href="/CS109a_DataScience_UserRatings_Team_Project/ourmodels.html">Our Models</a></li>
      <li><a href="/CS109a_DataScience_UserRatings_Team_Project/references.html">References</a></li>
    </ul>

    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project">View on GitHub</a>

          <h1 id="project_title">CS-109a - Intro to Datascience Team Project</h1>
          <h2 id="project_tagline">More on our Models</h2>


            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/starbuck10/CS109a_DataScience_UserRatings_Team_Project/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Continued Exploration</h3>

<p>Explanation and graphs, etc.</p>

<p>This is how we <code>blanked</code> blank blank blank:</p>
<pre><code>Insert code here...
  and here.
      and here.
</code></pre>

<p>When visualized, in the form of a line graph, this is what we see: </p>

<div style="display: flex; justify-content: center;">
  <img src="images/randomplaceholdergraph.png", alt="Placeholder Graph", style="width:600px;height:400px;" />
</div>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model #1</h3>

<p>Explanation and graphs, etc.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model #2</h3>

<p>Explanation and graphs, etc.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model #3</h3>

<p>Explanation and graphs, etc.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model #4</h3>

<p>Explanation and graphs, etc.</p>

        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluating our Winning Model!</h3>

<p>In case you missed it, our winning model for the project used <strong>blending of baseline values and k-nn (k=30) with item effects (movie similarity)</strong>, where baseline values accounted for 7% of the datapoints.  This model gave us our highest test score of .3045 and a Root Mean Square Error (RMSE) of .8776, taking 5 sec to initiate the effects, 1 min 21 sec to fit the data, and 11 min to score the model.</p>
<p>Now that we have the model that seems to best represent our dataset, let's go through the process of understanding how to effectively rate a recommendation engine.  If you think back to the approach page, here are the areas we need to analyze our model, along with how these areas are typically analyzed (even if we don't have the ability to explore the model this in depth for the project, it's good for us to go through the motions of understanding next steps toward implementation):</p>

<p>1. <strong>User Preference-</strong> typically determined by user interviews and surveys.  Simply posing the question, "which model do you prefer?" </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- This process is out of scope for the project but a very interesting and essential test of any model - let's run this by users and see if their gut reactions and feelings/preferences match what the data are telling us.</p>
<p>2. <strong>Prediction Accuracy-</strong> normally measured with R^2 or a similar error explanation term, this can also take into account % classified properly (within  given margin).</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- In this case, let's compare our performance against the baseline as well as our scoring.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- Compared to the baseline, we've ... (R^2)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;- Next question - were the gains worth the processing time and effort to build and implement the model?</p>
<p>3. <strong>Coverage-</strong> measured by the percentage of all items that can ever be recommended given our dataset.  For instance, if our model can only correctly predict 25% of the dataset, our coverage is only 25%.  And in that example, we would only use the model to predict that portion of the dataset (i.e., not recommend movies outside of our model's capability and not attempt a recommendation on a similar movie/rating/user who may not fall within those guidelines as well)</p>
<p>4. <strong>Confidence-</strong> (the system’s trust in its recommendations or predictions, which can grow with the amount of data it has to work with)</p>
<p>5. <strong>Trust-</strong> (the user’s trust in the system’s recommendations, which may lead us to introduce prior beliefs to lead the system to recommend “safer” options for users so that it is “right” more often than not)</p>
<p>6. <strong>Novelty-</strong> (recommendations for items that the user did not know about)</p>
<p>7. <strong>Serendipity-</strong> (how surprising the successful recommendations are to users)</p>
<p>8. <strong>Diversity-</strong> (a measure of item-to-item similarity – recommending the same movie a user likes, for instance, isn’t particularly helpful)</p>
<p>9. <strong>Utility-</strong> (the expected utility of the recommendation – does this recommendation engine result in the service being used more, for instance)</p>
<p>10. <strong>Risk-</strong> (minimizing risk in recommending items)</p>
<p>11. <strong>Robustness-</strong> (the stability of the recommendation in the presence of fake information)</p>
<p>12. <strong>Privacy-</strong> (no disclosure of private information from the user)</p>
<p>13. <strong>Adaptivity-</strong> (how well the model adapts to changing title availability and trends in preference)</p>

<h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Continuing our findings</h3>

<p>To continue working through this dataset to find the ultimate model, we should consider the following approaches:</p>

<p>1. <strong>Idea #1-</strong> what this is and might mean</p>
<p>2. <strong>Idea #2-</strong> what this is and might mean</p>
<p>3. <strong>Remove additional global effects-</strong> just as we pulled out user and movie effects to build our models, we could continue doing the same process with actors, directors, genre, and more.  We could emulate the rating using temporal information (ratings will decline as movies get older, for instance).  This proces could reveal some interesting information for our modeling process.</p>



      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">This page maintained by <a href="https://github.com/starbuck10">starbuck10</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a> &copy; 2016</p>
      </footer>
    </div>



  </body>
</html>
